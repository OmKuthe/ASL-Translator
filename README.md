# ðŸ¤Ÿ ASL Translator â€“ Real-Time Sign Language Recognition

A real-time American Sign Language (ASL) translator that converts sign gestures into complete English sentences using MediaPipe for hand tracking and gesture classification. The system implements rule-based logic to form grammatically correct sentences from recognized gestures, making it an intuitive communication tool for basic ASL interpretation.

---

## âœ¨ Key Features

### Real-Time Processing
- ðŸ–ï¸ Accurate hand tracking using MediaPipe's Hand Landmarks
- âš¡ Smooth webcam-based gesture detection
- ðŸŽ­ Background-agnostic detection (works in varied environments)

### Gesture Recognition
- ðŸ”„ Sequence buffer (30-frame window) for stable recognition
- ðŸ“Š Confidence thresholding to ensure reliable predictions
- âœ¨ Normalized coordinates for position/scale invariance

### User Experience
- ðŸ“ Dynamic sentence construction with rule-based grammar
- ðŸŽ¯ Visual feedback (prediction buffer, status indicators)
- â³ Auto-clearing of displayed sentences after delay
- ðŸŽ¨ Clean UI overlay with OpenCV visualization

---

## ðŸ› ï¸ Technical Implementation

### Core Technologies

| Component          | Technology Used     |
|--------------------|---------------------|
| Hand Tracking      | MediaPipe Hands     |
| Classification     | Keras Model (Dense) |
| Computer Vision    | OpenCV              |
| Data Processing    | NumPy, JSON         |

> ðŸ“Œ Note: This project uses a **frame-wise gesture classifier** trained on normalized keypoints â€” not an LSTM-based sequence model.

---

## ðŸ“‹ Supported Gestures & Sentences

### Recognized Vocabulary (15 Signs)

["what", "your", "name", "my", "is", 
 "how", "you", "are", "fine", "where",
 "help", "please", "I", "want", "food"]
### Smart Sentence Formation

| Gesture Sequence       | Output Sentence          |
|------------------------|--------------------------|
| what â†’ you â†’ name      | What is your name?       |
| you â†’ fine             | Are you fine?            |
| I â†’ fine               | I am fine.               |
| I â†’ want â†’ food        | I want food.             |
| where â†’ you            | Where are you?           |
| I â†’ help               | I need help!             |
| what â†’ you â†’ want      | Do you need something?   |

---

### ðŸš€ Getting Started

### Installation

```bash
pip install opencv-python mediapipe tensorflow matplotlib scikit-learn
```

## Workflow

### ðŸ“¦ Data Collection

```bash
python data_collection.py
```
Press number keys (0â€“9) and letters (aâ€“e) to record each gesture

Saves data to data/X.npy and data/y.npy

ðŸ§  Model Training
```bash
python model_training.py
```
Trains the gesture classifier

Saves model as models/lstm_model.keras (name kept for compatibility)

ðŸŽ¥ Real-Time Translation
```bash
python predict_realtime.py
Uses webcam feed to detect gestures and display sentences
```
ðŸ“‚ Project Structure

```bash
ASL-Translator/
â”œâ”€â”€ data/                   # Collected gesture datasets
â”‚   â”œâ”€â”€ X.npy               # Feature vectors
â”‚   â””â”€â”€ y.npy               # Corresponding labels
â”œâ”€â”€ models/                 # Trained model & label maps
â”‚   â”œâ”€â”€ lstm_model.keras    # Gesture classifier
â”‚   â””â”€â”€ label_map.json      # Gesture-to-index mapping
â”œâ”€â”€ data_collection.py      # Gesture recording script
â”œâ”€â”€ model_training.py       # Model training pipeline
â”œâ”€â”€ predict_realtime.py     # Live translation system
â””â”€â”€ README.md               # Documentation
```
